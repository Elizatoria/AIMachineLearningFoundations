{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the necessary Python libaries using 'pip install'\n",
    "This can be done in the terminal as well.\n",
    "\n",
    "* python-dotenv - We're using it to store our OpenAI API key safely\n",
    "* openai - We're using it to make API calls to OpenAI to use their models/services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries into our script\n",
    "* We need the load_dotenv function from the dotenv library to access the variables (our API key) we've set in our separate .env file\n",
    "    * This takes the API key from the .env file and stores it on the device as an \"environment variable\"\n",
    "* We use the os library to then call the variable (our API key) using os.getenv() after calling the load_dotenv function\n",
    "    * This is how we can access the API key from the device's \"environment variables\" and use it in our code without needing to hard code the API key\n",
    "    * Ex. of hard coding that would make your API key available to anyone who accesses this code\n",
    "        * api_key = 'sk-fakeapikey-dn467JHGG@3^njsu99&0'\n",
    "* We us OpenAI from the openai library to make the connection between our code and the API\n",
    "    * This is where we enter the API key to gain access to OpenAI's models/services\n",
    "\n",
    "I have the print statement here to show that before using the load_dotenv function, there is nothing set to this environment variable.\n",
    "It should just print 'None'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the API key as an environment variable\n",
    "* When using the load_dotenv function, you should pass the filename of the .env file in as an argument\n",
    "    * If you still get 'None' from the print statement, I'd suggest right-clicking on the .env file, clicking 'Copy Path', and pasting the full filepath in as an argument to the load_dotenv function.\n",
    "        * Remember the argument needs to be a string (\"filepath\") and if it is the full filepath, change the '\\' characters to '/'\n",
    "* os.getenv(\"OPENAI_API_KEY\") should now be the API key since we called load_dotenv, so lets test it with a print statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-test1-csAGCoGDWNK85hAMuJBmT3BlbkFJNtYmDLw5z5jNCqRjYh8k\n"
     ]
    }
   ],
   "source": [
    "load_dotenv('api.env') # Load the environment variables from the .env file\n",
    "print(os.getenv(\"OPENAI_API_KEY\")) # Print the value of the OPENAI_API_KEY variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the connection between the code and OpenAI's API\n",
    "Now that we have the API key safely imported into our code, lets give OpenAI our API key so we can access their models/services.\n",
    "'client' is now our connection to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the API call\n",
    "API calls or API requests are when we ask the provider (OpenAI in this case) for their data/service. \n",
    "* There will be specific endpoints for each service or branch of data (client.chat.completions.create() is the endpoint that generates an AI response to a conversation)\n",
    "* There will be parameters (both required and optional) that we fill out to detail the request\n",
    "    * Required:\n",
    "        * model: This is which model we want to respond to our conversation (ex. 'gpt-3.5-turbo' or 'gpt-4o')\n",
    "        * messages: This is the conversation we want the model to respond to. This is also considered the context.\n",
    "            * We format the conversation in a list of dictionaries where each dictionary holds a part of the conversation.\n",
    "    * Optional:\n",
    "        * max_tokens: This is the maximum amount of tokens allowed for the AI response. \n",
    "            * This will cut the response off mid sentence at times, but is helpful for reducing costs and shortening responses to simple prompts.\n",
    "            * The output is also limited by the context limit. Each model has a different limit (8k-128k tokens). This limit accounts for both the input and output tokens.\n",
    "        * temperature: This controls the \"creativity\" of the model. Adds some variance to the output. Ranges from 0-2.\n",
    "            * 0: Focused and deterministic. Closest representation of the training data.\n",
    "            * 1: This is the default value that the temperature parameter is set to.\n",
    "            * 2: More random and \"creative\". Sometimes can result in jibberish. Fun to expirement with. \n",
    "        * stream: This takes a boolean value (True or False) on wether the output should be streamed token-by-token or displayed all together when the generation has fully completed.\n",
    "        * n: Takes an integer (defaults to 1). This determines how many responses will be generated. It's a way of creating multiple options for responses, but also multiplies the output cost by n.\n",
    "        * A couple other parameters that are more technical which you can read about here: https://platform.openai.com/docs/api-reference/chat/create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user input as a prompt\n",
    "prompt = input(\"Enter a prompt: \")\n",
    "\n",
    "# Format it within a list of dictionaries to be passed to the API\n",
    "# role: This is basically the speaker of the message. It can be either \"user\" (the user) or \"system\" (the system message or character description for the AI) or \"assistant\" (the model's response).\n",
    "# content: This is the actual message that the speaker is saying.\n",
    "\n",
    "message_list = [\n",
    "    # This is the system message. You can add more detail here about how you want the model to respond.\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    # This is the user message. This is the prompt that you want the model to respond to.\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the API\n",
    "# This is the bare necessity for making a chat completions API call to OpenAI\n",
    "# 'response' will contain the model's response to the prompt\n",
    "response = client.chat.completions.create(\n",
    "    model='gpt-3.5-turbo', # model name\n",
    "    messages=message_list, # messages list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigate/Read the response object\n",
    "* 'response' will contain the model's response to the prompt\n",
    "    * choices: A list that holds the \"choices\" of the model. There can be multiple choices based on if you set 'n' to more than 1 within the API call.\n",
    "        * finish_reason: This signifies the reason the model response finished. This is normally either 'stop' or 'length'.\n",
    "            * 'stop' means it finished with it's response how it should normally\n",
    "            * 'length' means the response got limited by either the 'max_tokens' parameter in your API call, or you hit the context limit of the model you're using.\n",
    "        * index: This is the index of choices that you're looking at. Usually is 0 (due to zero-based indexing) unless you set 'n' to more than 1 within the API call.\n",
    "        * message: This holds the response message of the model.\n",
    "            * content: The actual text content of the message.\n",
    "            * role: The speaker of the message. 100% always going to be 'assistant' coming from the model.\n",
    "            * function_call: We will be getting into this next class\n",
    "            * tool_calls: We will be getting into this next class\n",
    "    * model: What model was used to generate this response. This should be the same as the model name we specified in the API call.\n",
    "    * usage: Shows us how many tokens were used for our API call.\n",
    "        * completion_tokens: This is how many tokens were used on the output from the model.\n",
    "            * If max_tokens was set in the API call, that will be the max amount that completion_tokens will reach.\n",
    "        * prompt_tokens: This is how many tokens were used for the input. This is the token count for our message list we have in our API call.\n",
    "        * total_tokens: The sum of the prompt_tokens and the completion_tokens. A helpful value to check to track your API usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9eRIuR7uK0pHm7w4fjAZT6VqCnScR', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Sure, here's one for you:\\n\\nWhy don't some couples go to the gym?\\n\\nBecause some relationships don't work out!\", role='assistant', function_call=None, tool_calls=None))], created=1719425104, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=26, prompt_tokens=21, total_tokens=47))\n"
     ]
    }
   ],
   "source": [
    "# This prints out the whole response object with all the details\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:\n",
      "\n",
      "Sure, here's one for you:\n",
      "\n",
      "Why don't some couples go to the gym?\n",
      "\n",
      "Because some relationships don't work out!\n",
      "\n",
      "\n",
      "Total tokens used: 47\n"
     ]
    }
   ],
   "source": [
    "# Here we navigate through the response object to get the model's response and the total tokens the API call used\n",
    "print(f'Response:\\n\\n{response.choices[0].message.content}\\n\\n')\n",
    "print(f'Total tokens used: {response.usage.total_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")  # Load the environment variables from the .env file\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY')) # Initialize the OpenAI client\n",
    "prompt = input(\"Enter a message: \") # Take the user input as a prompt\n",
    "# Format it within a list of dictionaries to be passed to the API\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"You are a helpful assistant.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "# Set stream to True to stream the response from the model\n",
    "stream = True\n",
    "\n",
    "# Call the API\n",
    "# I also added temperature set to 0.0 to get the most accurate response\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages,\n",
    "    temperature=0.0,\n",
    "    stream=stream # Stream the response\n",
    ")\n",
    "\n",
    "# Print the response (streamed or not streamed)\n",
    "if stream:\n",
    "    # Stream the response\n",
    "    output_content = '' # Creates an empty string to store the output content in it's entirety\n",
    "    # iterates through the completion object to get the content of the response chunk by chunk \n",
    "    for chunk in completion:\n",
    "        # Checks if the chunk is a message\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            # Prints the content of the chunk\n",
    "            print(chunk.choices[0].delta.content, end=\"\")\n",
    "            # Adds the content of the chunk to the output_content string\n",
    "            output_content += chunk.choices[0].delta.content\n",
    "else:\n",
    "    # Prints the content of the completion object if it's not streamed\n",
    "    answer = completion.choices[0].message.content\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Exercise: Call the Chat Completions API to generate a joke using GPT 3.5 Turbo as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award? Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")  # Load the environment variables from the .env file\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get('OPENAI_API_KEY')) # Initialize the OpenAI client\n",
    "\n",
    "# Format it within a list of dictionaries to be passed to the API\n",
    "# Here we know what the user is going to say, so we can add it right into the messages list\n",
    "# Also for simple prompts like this, we don't need to add a system message to the messages list\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"Tell me a joke.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call the API - make sure we are using the gpt-3.5-turbo model\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# Print the response message\n",
    "print(completion.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
